{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":""},{"location":"#xavier-exome-analysis-and-variant-explorer","title":"XAVIER - eXome Analysis and Variant explorER \ud83d\udd2c","text":"<p>_XAVIER - eXome Analysis and Variant explorER_. This is the home of the pipeline, XAVIER. Its long-term goals: to accurately call germline and somatic variants, to infer CNVs, and to boldly annotate variants like no pipeline before!</p>"},{"location":"#overview","title":"Overview","text":"<p>Welcome to XAVIER! Before getting started, we highly recommend reading through xavier's documentation.</p> <p>The <code>xavier</code> pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions:</p> <ul> <li><code>xavier run</code>: Run the XAVIER pipeline with your input files.</li> <li><code>xavier unlock</code>: Unlocks a previous runs output directory.</li> <li><code>xavier cache</code>: Cache remote resources locally, coming soon!</li> </ul> <p>XAVIER is a comprehensive whole exome-sequencing pipeline following the Broad's set of best practices. It relies on technologies like Singularity<sup>1</sup> to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake<sup>2</sup>, a flexible and scalable workflow management system, to submit jobs to a cluster or cloud provider.</p> <p>The pipeline is compatible with data generated from Illumina short-read sequencing technologies. As input, it accepts a set of FastQ or BAM files and can be run locally on a compute instance, on-premise using a cluster, or on the cloud (feature coming soon!). A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM, or run on AWS using Tibanna (feature coming soon!). A hybrid approach ensures the pipeline is accessible to all users.</p> <p>Before getting started, we highly recommend reading through the usage section of each available sub command.</p> <p>For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github.</p>"},{"location":"#dependencies","title":"Dependencies","text":"<p>Requires: <code>singularity&gt;=3.5</code> <code>snakemake==6.X</code></p> <p>Snakemake and singularity must be installed on the target system. Snakemake orchestrates the execution of each step in the pipeline. To guarantee the highest level of reproducibility, each step relies on versioned images from DockerHub. Snakemake uses singaularity to pull these images onto the local filesystem prior to job execution, and as so, snakemake and singularity are the only two dependencies.</p>"},{"location":"#run-xavier-pipeline","title":"Run XAVIER pipeline","text":""},{"location":"#biowulf","title":"Biowulf","text":"<pre><code># XAVIER is configured to use different execution backends: local or slurm\n# view the help page for more information\nmodule load ccbrpipeliner\nxavier run --help\n\n# @slurm: uses slurm and singularity execution method\n# The slurm MODE will submit jobs to the cluster.\n# It is recommended running XAVIER in this mode.\n\n# Please note that you can dry-run the command below\n# by providing  --runmode dryrun flag\n\n# Do not run this on the head node!\n# Grab an interactive node\nsinteractive --mem=110g --cpus-per-task=12 --gres=lscratch:200\nmodule load ccbrpipeliner\n\n# First, initialize the output directory\nxavier run \\\n--input data/*.R?.fastq.gz \\\n--output /data/$USER/xavier_hg38 \\\n--genome hg38 \\\n--pairs pairs.txt \\\n--targets resources/Agilent_SSv7_allExons_hg38.bed \\\n--mode slurm \\\n--runmode init\n\n# Second, do a dry run to visualize outputs\nxavier run \\\n--input data/*.R?.fastq.gz \\\n--output /data/$USER/xavier_hg38 \\\n--genome hg38 \\\n--pairs pairs.txt \\\n--targets resources/Agilent_SSv7_allExons_hg38.bed \\\n--mode slurm \\\n--runmode dryrun\n\n# Then do a complete run\nxavier run \\\n--input data/*.R?.fastq.gz \\\n--output /data/$USER/xavier_hg38 \\\n--genome hg38 \\\n--pairs pairs.txt \\\n--targets resources/Agilent_SSv7_allExons_hg38.bed \\\n--mode slurm \\\n--runmode run\n</code></pre>"},{"location":"#frce","title":"FRCE","text":"<pre><code># grab an interactive node\nsrun --export all --pty --x11 bash\n\n# add xavier to path correctly\n. /mnt/projects/CCBR-Pipelines/pipelines/guis/latest/bin/setup\n\n# add SIF cache and TMPDIR path\nSIFCACHE=\"/mnt/projects/CCBR-Pipelines/SIFs/XAVIER\"\nTMPDIR=\"/scratch/cluster_scratch/$USER\"\n\n# run xavier\n\n# Initialize and then dryrun (or run)\nxavier run \\\n--input data/*.R?.fastq.gz \\\n--output /data/$USER/xavier_hg38 \\\n--genome hg38 \\\n--sif-cache $SIFCACHE \\\n--tmp-dir $TMPDIR \\\n--pairs pairs.txt \\\n--targets resources/Agilent_SSv7_allExons_hg38.bed \\\n--mode slurm \\\n--runmode init # run\n</code></pre>"},{"location":"#contribute","title":"Contribute","text":"<p>This site is a living document, created for and by members like you. XAVIER is maintained by the members of CCBR and is improved by continuous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our repository.</p>"},{"location":"#references","title":"References","text":"<p><sup>1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459.</sup> <sup>2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.</sup></p>"},{"location":"contributing/","title":"Contributing to XAVIER","text":""},{"location":"contributing/#proposing-changes-with-issues","title":"Proposing changes with issues","text":"<p>If you want to make a change, it's a good idea to first open an issue and make sure someone from the team agrees that it\u2019s needed.</p> <p>If you've decided to work on an issue, assign yourself to the issue so others will know you're working on it.</p>"},{"location":"contributing/#pull-request-process","title":"Pull request process","text":"<p>We use GitHub Flow as our collaboration process. Follow the steps below for detailed instructions on contributing changes to XAVIER.</p> <p></p>"},{"location":"contributing/#clone-the-repo","title":"Clone the repo","text":"<p>If you are a member of CCBR, you can clone this repository to your computer or development environment. Otherwise, you will first need to fork the repo and clone your fork. You only need to do this step once.</p> <pre><code>git clone https://github.com/CCBR/XAVIER\n</code></pre> <p>Cloning into 'XAVIER'...  remote: Enumerating objects: 1136, done.  remote: Counting objects: 100% (463/463), done.  remote: Compressing objects: 100% (357/357), done.  remote: Total 1136 (delta 149), reused 332 (delta 103), pack-reused 673  Receiving objects: 100% (1136/1136), 11.01 MiB | 9.76 MiB/s, done.  Resolving deltas: 100% (530/530), done. </p> <pre><code>cd XAVIER\n</code></pre>"},{"location":"contributing/#if-this-is-your-first-time-cloning-the-repo-you-may-need-to-install-dependencies","title":"If this is your first time cloning the repo, you may need to install dependencies","text":"<ul> <li> <p>Install snakemake and singularity or docker if needed (biowulf already has these available as modules).</p> </li> <li> <p>Install the python dependencies with pip</p> </li> </ul> <pre><code>pip install .\n</code></pre> <p>If you're developing on biowulf, you can use our shared conda environment which already has these dependencies installed</p> <pre><code>. \"/data/CCBR_Pipeliner/db/PipeDB/Conda/etc/profile.d/conda.sh\"\nconda activate py311\n</code></pre> <ul> <li>Install <code>pre-commit</code> if you don't already   have it. Then from the repo's root directory, run</li> </ul> <pre><code>pre-commit install\n</code></pre> <p>This will install the repo's pre-commit hooks.   You'll only need to do this step the first time you clone the repo.</p>"},{"location":"contributing/#create-a-branch","title":"Create a branch","text":"<p>Create a Git branch for your pull request (PR). Give the branch a descriptive name for the changes you will make, such as <code>iss-10</code> if it is for a specific issue.</p> <pre><code># create a new branch and switch to it\ngit branch iss-10\ngit switch iss-10\n</code></pre> <p>Switched to a new branch 'iss-10'</p>"},{"location":"contributing/#make-your-changes","title":"Make your changes","text":"<p>Edit the code, write and run tests, and update the documentation as needed.</p>"},{"location":"contributing/#test","title":"test","text":"<p>Changes to the python package code will also need unit tests to demonstrate that the changes work as intended. We write unit tests with pytest and store them in the <code>tests/</code> subdirectory. Run the tests with <code>python -m pytest</code>.</p> <p>If you change the workflow, please run the workflow with the test profile and make sure your new feature or bug fix works as intended.</p>"},{"location":"contributing/#document","title":"document","text":"<p>If you have added a new feature or changed the API of an existing feature, you will likely need to update the documentation in <code>docs/</code>.</p>"},{"location":"contributing/#commit-and-push-your-changes","title":"Commit and push your changes","text":"<p>If you're not sure how often you should commit or what your commits should consist of, we recommend following the \"atomic commits\" principle where each commit contains one new feature, fix, or task. Learn more about atomic commits here: https://www.freshconsulting.com/insights/blog/atomic-commits/</p> <p>First, add the files that you changed to the staging area:</p> <pre><code>git add path/to/changed/files/\n</code></pre> <p>Then make the commit. Your commit message should follow the Conventional Commits specification. Briefly, each commit should start with one of the approved types such as <code>feat</code>, <code>fix</code>, <code>docs</code>, etc. followed by a description of the commit. Take a look at the Conventional Commits specification for more detailed information about how to write commit messages.</p> <pre><code>git commit -m 'feat: create function for awesome feature'\n</code></pre> <p>pre-commit will enforce that your commit message and the code changes are styled correctly and will attempt to make corrections if needed.</p> <p>Check for added large files..............................................Passed  Fix End of Files.........................................................Passed  Trim Trailing Whitespace.................................................Failed </p> <ul> <li>hook id: trailing-whitespace </li> <li>exit code: 1 </li> <li>files were modified by this hook  &gt;    Fixing path/to/changed/files/file.txt  &gt;    codespell................................................................Passed    style-files..........................................(no files to check)Skipped    readme-rmd-rendered..................................(no files to check)Skipped    use-tidy-description.................................(no files to check)Skipped </li> </ul> <p>In the example above, one of the hooks modified a file in the proposed commit, so the pre-commit check failed. You can run <code>git diff</code> to see the changes that pre-commit made and <code>git status</code> to see which files were modified. To proceed with the commit, re-add the modified file(s) and re-run the commit command:</p> <pre><code>git add path/to/changed/files/file.txt\ngit commit -m 'feat: create function for awesome feature'\n</code></pre> <p>This time, all the hooks either passed or were skipped (e.g. hooks that only run on R code will not run if no R files were committed). When the pre-commit check is successful, the usual commit success message will appear after the pre-commit messages showing that the commit was created.</p> <p>Check for added large files..............................................Passed  Fix End of Files.........................................................Passed  Trim Trailing Whitespace.................................................Passed  codespell................................................................Passed  style-files..........................................(no files to check)Skipped  readme-rmd-rendered..................................(no files to check)Skipped  use-tidy-description.................................(no files to check)Skipped  Conventional Commit......................................................Passed  &gt; [iss-10 9ff256e] feat: create function for awesome feature  1 file changed, 22 insertions(+), 3 deletions(-) </p> <p>Finally, push your changes to GitHub:</p> <pre><code>git push\n</code></pre> <p>If this is the first time you are pushing this branch, you may have to explicitly set the upstream branch:</p> <pre><code>git push --set-upstream origin iss-10\n</code></pre> <p>Enumerating objects: 7, done.  Counting objects: 100% (7/7), done.  Delta compression using up to 10 threads  Compressing objects: 100% (4/4), done.  Writing objects: 100% (4/4), 648 bytes | 648.00 KiB/s, done.  Total 4 (delta 3), reused 0 (delta 0), pack-reused 0  remote: Resolving deltas: 100% (3/3), completed with 3 local objects.  remote:  remote: Create a pull request for 'iss-10' on GitHub by visiting:  remote: https://github.com/CCBR/XAVIER/pull/new/iss-10  remote:  To https://github.com/CCBR/XAVIER  &gt;  &gt; [new branch] iss-10 -&gt; iss-10  branch 'iss-10' set up to track 'origin/iss-10'. </p> <p>We recommend pushing your commits often so they will be backed up on GitHub. You can view the files in your branch on GitHub at <code>https://github.com/CCBR/XAVIER/tree/&lt;your-branch-name&gt;</code> (replace <code>&lt;your-branch-name&gt;</code> with the actual name of your branch).</p>"},{"location":"contributing/#create-the-pr","title":"Create the PR","text":"<p>Once your branch is ready, create a PR on GitHub: https://github.com/CCBR/XAVIER/pull/new/</p> <p>Select the branch you just pushed:</p> <p></p> <p>Edit the PR title and description. The title should briefly describe the change. Follow the comments in the template to fill out the body of the PR, and you can delete the comments (everything between <code>&lt;!--</code> and <code>--&gt;</code>) as you go. Be sure to fill out the checklist, checking off items as you complete them or striking through any irrelevant items. When you're ready, click 'Create pull request' to open it.</p> <p></p> <p>Optionally, you can mark the PR as a draft if you're not yet ready for it to be reviewed, then change it later when you're ready.</p>"},{"location":"contributing/#wait-for-a-maintainer-to-review-your-pr","title":"Wait for a maintainer to review your PR","text":"<p>We will do our best to follow the tidyverse code review principles: https://code-review.tidyverse.org/. The reviewer may suggest that you make changes before accepting your PR in order to improve the code quality or style. If that's the case, continue to make changes in your branch and push them to GitHub, and they will appear in the PR.</p> <p>Once the PR is approved, the maintainer will merge it and the issue(s) the PR links will close automatically. Congratulations and thank you for your contribution!</p>"},{"location":"contributing/#after-your-pr-has-been-merged","title":"After your PR has been merged","text":"<p>After your PR has been merged, update your local clone of the repo by switching to the main branch and pulling the latest changes:</p> <pre><code>git checkout main\ngit pull\n</code></pre> <p>It's a good idea to run <code>git pull</code> before creating a new branch so it will start from the most recent commits in main.</p>"},{"location":"contributing/#helpful-links-for-more-information","title":"Helpful links for more information","text":"<ul> <li>GitHub Flow</li> <li>semantic versioning guidelines</li> <li>changelog guidelines</li> <li>tidyverse code review principles</li> <li>reproducible examples</li> <li>nf-core extensions for VS Code</li> </ul>"},{"location":"license/","title":"MIT License","text":"<p>Copyright \u00a9 2021 CCBR</p> <p><sub>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</sub></p> <p><sub>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</sub></p> <p><sub>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</sub></p>"},{"location":"release-guide/","title":"Release Guide","text":"<p>Make sure you're keeping the changelog up-to-date during development. Ideally, every PR that includes a user-facing change (e.g. a new feature, bug fix, or any API change) should add a concise summary to the changelog with a link to the PR. Only approve or merge PRs that either update the changelog or have no user-facing changes.</p>"},{"location":"release-guide/#how-to-release-a-new-version-on-github","title":"How to release a new version on GitHub","text":"<ol> <li>Determine the new version number according to semantic versioning guidelines.</li> <li>Update <code>CHANGELOG.md</code>:</li> <li>Edit the heading for the development version to match the new version.</li> <li>If needed, clean up the changelog -- fix any typos, optionally create subheadings for 'New features' and 'Bug fixes' if there are lots of changes, etc.</li> <li>Update the version in <code>src/__init__.py</code>.</li> <li>On GitHub, go to \"Releases\" and click \"Draft a new release\". https://github.com/CCBR/XAVIER/releases/new</li> <li>Choose a tag: same as the version number.</li> <li>Choose the target: most likely this should be the main branch, or a specific commit hash.</li> <li>Set the title as the new version number, e.g. v3.0.2</li> <li>Copy and paste the release notes from the CHANGELOG into the description box.</li> <li>Check the box \"Set as the latest release\".</li> <li>Click \"Publish release\".</li> <li>Post release chores:</li> <li>Add a new \"development version\" heading to the top of <code>CHANGELOG.md</code>.</li> <li>Bump the version number in <code>src/__init__.py</code> to include <code>-dev</code>, e.g. <code>v3.0.2-dev</code> if you just released <code>v3.0.2</code>.</li> </ol>"},{"location":"release-guide/#how-to-install-a-release-on-biowulf","title":"How to install a release on biowulf","text":"<p>After releasing a new version on GitHub:</p> <pre><code># go to the shared pipeline directory on biowulf\ncd /data/CCBR_Pipeliner/Pipelines/XAVIER\n\n# clone the new version tag (e.g. v3.0.2) to a hidden directory\ngit clone --depth 1 --branch v3.0.2 https://github.com/CCBR/XAVIER .v3.0.2\n\n# change permissions for the new directory so anyone will be able to use the pipeline\nchown -R :CCBR_Pipeliner .v3.0.2\nchmod -R a+rX /data/CCBR_Pipeliner/Pipelines/XAVIER/.v3.0.2\n\n# if needed, remove the old symlink for the minor version number\nrm -i v3.0\n\n# recreate the symlink to point to the new latest version\nln -s .v3.0.2 v3.0\n\n# you can verify that the symlink points to the new version with readlink\nreadlink -f v3.0\n</code></pre> <p>Versions of the <code>ccbrpipeliner</code> module only specify the major and minor version of each pipeline. If the new pipeline release only increments the patch number, <code>ccbrpipeliner</code> will use it automatically after you update the symlink as above. If you need to release a new major or minor version of a pipeline on biowulf, contact Kelly or Vishal.</p> <p>Verify that <code>ccbrpipeliner</code> uses the latest version with:</p> <pre><code>module load ccbrpipeliner &amp;&amp; xavier --version\n</code></pre>"},{"location":"faq/questions/","title":"Frequently Asked Questions","text":"<p>Coming soon!</p>"},{"location":"pipeline-details/methods/","title":"Methods description","text":"<p>This page contains a description of all methods used in the pipeline, along with references for important tools.</p> <p>Note that depending on the settings used, not all of these methods may be applicable, so please adapt this text appropriately for your application.</p> <p>You can also download this text as a Word document (.docx) that contains an EndNote traveling library using the button below.</p> <p></p>"},{"location":"pipeline-details/methods/#data-preprocessing","title":"Data preprocessing","text":"<p>Low-quality and adapters sequences are trimmed from the raw sequencing reads using Trimmomatic (v. 0.39)<sup>1</sup>. Trimmed reads are then aligned to the human hg38 reference genome using BWA mapping software (v. 0.7.17)<sup>2</sup>. Duplicate reads are marked using Samblaster (v. 0.1.25)<sup>3</sup> and sorted using samtools (v. 1.8). Finally, base quality score recalibration is performed as indicated in the GATK4 (v. 4.2.2.0) best practices <sup>4</sup>.</p>"},{"location":"pipeline-details/methods/#germline-variant-calling","title":"Germline variant calling","text":"<p>HaplotypeCaller from GATK4 (v. 4.2.2.0) is used to call germline variants, parallelized across chromosomes, and all samples in the cohort are joint genotyped together <sup>4</sup><sup>,</sup><sup>5</sup>.</p>"},{"location":"pipeline-details/methods/#somatic-variant-calling","title":"Somatic variant calling","text":"<p>Somatic variant calling (SNPs and Indels) is performed using Mutect (v. 1.1.7)<sup>6</sup>, Mutect2 (GATK v. 4.2.0)<sup>7</sup>, Strelka2 (v. 2.9.0)<sup>8</sup>, and VarDict (v. 1.4)<sup>9</sup> in tumor-normal mode. Variants from all callers are merged using the CombineVariants tool from GATK version 3.8-1. Genomic, functional and consequence annotations are added using Variant Effect Predictor (VEP v. 99)<sup>10</sup> and converted to Mutation Annotation Format (MAF) using the vcf2maf tool (v. 1.6.16)<sup>11</sup>.</p> <p>For Copy Number Variants (CNVs), Control-Freec (v. 11.6)<sup>12</sup> is used to generate pileups, which are used as input for the R package 'sequenza' (v. 3.0.0)<sup>13</sup>. The complete Control-Freec workflow is then re-run using ploidy and cellularity estimates from 'sequenza'.</p>"},{"location":"pipeline-details/methods/#ffpe-artifact-filtering","title":"FFPE Artifact filtering","text":"<p>SOBDetector is a tool that scores variants based on strand-orientation bias, which can be a sign of DNA damage caused by fixation of tissue. This pipeline runs SOBDetector in a two-pass method. The first pass uses parameters provided with the software (calculated from publicly available data from TCGA), then cohort-specific bias metrics are computed from those results, and SOBDetector is re-run using these cohort-specific values.</p>"},{"location":"pipeline-details/methods/#quality-and-identity-metrics","title":"Quality and identity metrics","text":"<p>Ancestry and relatedness scores are generated using Somalier (v. 0.2.13)<sup>14</sup>. Contamination analyses are performed against viral and bacterial genomes from NCBI using Kraken2 (v. 2.1.2)<sup>15</sup>, as well as against mouse, human, and UniVec databases using FastQ Screen (v. 0.14.1)<sup>16</sup>. Sequence, mapping and variant statistics are computed using FastQC (v. 0.11.9), Qualimap (v. 2.2.1)<sup>17</sup> and SNPeff (v. 4.3t)<sup>18</sup>. All of these metrics are combined into an interactive HTML report using MultiQC (v. 1.11)<sup>19</sup>.</p>"},{"location":"pipeline-details/methods/#pipeline-orchestration","title":"Pipeline Orchestration","text":"<p>Job execution and management is done using Snakemake (v. 6.8.2)<sup>20</sup> using custom-built Singularity (v. 3.8.5) containers for reproducibility.</p>"},{"location":"pipeline-details/methods/#references","title":"References","text":"<ol> <li> <p>Bolger, A.M., M. Lohse, and B. Usadel, Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics, 2014. 30(15): p. 2114-20.\u00a0\u21a9</p> </li> <li> <p>Li, H. and R. Durbin, Fast and accurate short read alignment with Burrows-Wheeler transform. Bioinformatics, 2009. 25(14): p. 1754-60.\u00a0\u21a9</p> </li> <li> <p>Faust, G.G. and I.M. Hall, SAMBLASTER: fast duplicate marking and structural variant read extraction. Bioinformatics, 2014. 30(17): p. 2503-5.\u00a0\u21a9</p> </li> <li> <p>Van der Auwera, G.A. and B.D. O'Connor, Genomics in the cloud : using Docker, GATK, and WDL in Terra. First edition. ed. 2020, Sebastopol, CA: O'Reilly Media.\u00a0\u21a9\u21a9</p> </li> <li> <p>Poplin, R., et al., Scaling accurate genetic variant discovery to tens of thousands of samples. bioRxiv, 2018: p. 201178.\u00a0\u21a9</p> </li> <li> <p>Cibulskis, K., et al., Sensitive detection of somatic point mutations in impure and heterogeneous cancer samples. Nat Biotechnol, 2013. 31(3): p. 213-9.\u00a0\u21a9</p> </li> <li> <p>Benjamin, D., et al., Calling Somatic SNVs and Indels with Mutect2. bioRxiv, 2019: p. 861054.\u00a0\u21a9</p> </li> <li> <p>Kim, S., et al., Strelka2: fast and accurate calling of germline and somatic variants. Nat Methods, 2018. 15(8): p. 591-594.\u00a0\u21a9</p> </li> <li> <p>Lai, Z., et al., VarDict: a novel and versatile variant caller for next-generation sequencing in cancer research. Nucleic Acids Res, 2016. 44(11): p. e108.\u00a0\u21a9</p> </li> <li> <p>McLaren, W., et al., The Ensembl Variant Effect Predictor. Genome Biol, 2016. 17(1): p. 122.\u00a0\u21a9</p> </li> <li> <p>Memorial Sloan Kettering Cancer Center. vcf2maf. 2013; Available from: https://github.com/mskcc/vcf2maf.\u00a0\u21a9</p> </li> <li> <p>Boeva, V., et al., Control-FREEC: a tool for assessing copy number and allelic content using next-generation sequencing data. Bioinformatics, 2012. 28(3): p. 423-5.\u00a0\u21a9</p> </li> <li> <p>Favero, F., et al., Sequenza: allele-specific copy number and mutation profiles from tumor sequencing data. Ann Oncol, 2015. 26(1): p. 64-70.\u00a0\u21a9</p> </li> <li> <p>Pedersen, B. somalier: extract informative sites, evaluate relatedness, and perform quality-control on BAM/CRAM/BCF/VCF/GVCF. 2018; Available from: https://github.com/brentp/somalier.\u00a0\u21a9</p> </li> <li> <p>Wood, D.E., J. Lu, and B. Langmead, Improved metagenomic analysis with Kraken 2. Genome Biol, 2019. 20(1): p. 257.\u00a0\u21a9</p> </li> <li> <p>Wingett, S.W. and S. Andrews, FastQ Screen: A tool for multi-genome mapping and quality control. F1000Res, 2018. 7: p. 1338.\u00a0\u21a9</p> </li> <li> <p>Okonechnikov, K., A. Conesa, and F. Garcia-Alcalde, Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data. Bioinformatics, 2016. 32(2): p. 292-4.\u00a0\u21a9</p> </li> <li> <p>Cingolani, P., et al., A program for annotating and predicting the effects of single nucleotide polymorphisms, SnpEff: SNPs in the genome of Drosophila melanogaster strain w1118; iso-2; iso-3. Fly (Austin), 2012. 6(2): p. 80-92.\u00a0\u21a9</p> </li> <li> <p>Ewels, P., et al., MultiQC: summarize analysis results for multiple tools and samples in a single report. Bioinformatics, 2016. 32(19): p. 3047-8.\u00a0\u21a9</p> </li> <li> <p>Koster, J. and S. Rahmann, Snakemake-a scalable bioinformatics workflow engine. Bioinformatics, 2018. 34(20): p. 3600.\u00a0\u21a9</p> </li> </ol>"},{"location":"pipeline-details/output/","title":"Output files","text":""},{"location":"pipeline-details/output/#xavier","title":"XAVIER","text":"<p>The output files and their locations are broken down here for the XAVIER pipeline. Pre-processing and germline variant calling steps are common but somatic variant calling is dependent on whether the pipeline was run in either (A) tumor-normal pair or (B) tumor-only analysis mode. All file locations are relative to the output directory specified during the job submission.</p> <p>The output directory after a complete XAVIER run should look like:</p> <pre><code>xavier_output/\n\u251c\u2500\u2500 bams\n\u251c\u2500\u2500 cluster.json # cluster info for the run\n\u251c\u2500\u2500 config\n\u251c\u2500\u2500 config.json  # config file for the run\n\u251c\u2500\u2500 fastqs\n\u251c\u2500\u2500 germline\n\u251c\u2500\u2500 indels.vcf.gz[.tbi] # raw germline INDELs\n\u251c\u2500\u2500 input_files\n\u251c\u2500\u2500 intervals.list\n\u251c\u2500\u2500 {sample1}-normal.R1.fastq.gz -&gt; /path/to/{sample1}-normal.R1.fastq.gz\n\u251c\u2500\u2500 {sample1}-normal.R2.fastq.gz -&gt; /path/to/{sample1}-normal.R2.fastq.gz\n\u251c\u2500\u2500 {sample1}-tumor.R1.fastq.gz -&gt; /path/to/{sample1}-tumor.R1.fastq.gz\n\u251c\u2500\u2500 {sample1}-tumor.R2.fastq.gz -&gt; /path/to/{sample1}-tumor.R2.fastq.gz\n.\n.\n.\n\u251c\u2500\u2500 kickoff.sh\n\u251c\u2500\u2500 logfiles\n\u251c\u2500\u2500 QC\n\u251c\u2500\u2500 resources\n\u251c\u2500\u2500 snps.vcf.gz[.tbi] # raw germline SNPs\n\u251c\u2500\u2500 somatic_paired # in case of tumor-normal paired run\n\u251c\u2500\u2500 somatic_tumor_only # in case of tumor-only run\n\u2514\u2500\u2500 workflow\n</code></pre> <p>Below we describe the different folders that contain specific outputs obtained for all samples from the XAVIER pipeline</p>"},{"location":"pipeline-details/output/#1-qc","title":"1. <code>QC</code>","text":"<p>The <code>QC</code> folder contains all the Quality-Control analyses performed at different steps of the pipeline for each sample to assess sequencing quality before and after adapter trimming, microbial taxonomic composition, contamination, variant calling, etc. The final summary report and data is available <code>finalQC</code> folder. \\ The MultiQC report also contains results from other analysis like mapping statistics, ancestry and relatedness, etc. It is recommended to study the MultiQC report first to get a birds eye view of the sequence data quality.</p> <pre><code>QC/\n\u251c\u2500\u2500 exome_targets.bed\n\u251c\u2500\u2500 finalQC/\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 MultiQC_Report_data\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MultiQC_Report.html\n\u251c\u2500\u2500 FQscreen\n\u251c\u2500\u2500 sample1-normal\n\u251c\u2500\u2500 sample1-normal_fastqc.html\n.\n.\n.\n\u251c\u2500\u2500 sample1-tumor\n\u251c\u2500\u2500 sample1-tumor_fastqc.html\n.\n.\n.\n\u251c\u2500\u2500 kraken\n\u251c\u2500\u2500 raw_variants.het\n\u251c\u2500\u2500 raw_variants.variant_calling_detail_metrics\n\u2514\u2500\u2500 raw_variants.variant_calling_summary_metrics\n</code></pre>"},{"location":"pipeline-details/output/#2-bams","title":"2. <code>bams</code>","text":"<p>The <code>bams</code> folder contain two subfolders <code>chrom_split</code> and <code>final_bams</code>. <code>final_bams</code> contains the final processed BAM files for each sample in the run and the <code>chrom_split</code> folder contains all the sample BAM files split by each chromosome.</p> <pre><code>bams/\n\u251c\u2500\u2500 chrom_split\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 {sample1}-normal.chr1.split.bam[.bai]\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 {sample1}-normal.chr2.split.bam[.bai]\n.   .\n.   .\n.   .\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 {sampleN}-tumor.chrN.split.bam[.bai]\n\u2514\u2500\u2500 final_bams\n    \u251c\u2500\u2500 {sample1}-normal.bam[.bai]\n    \u251c\u2500\u2500 {sample1}-tumor.bam[.bai]\n    .\n    .\n    .\n    \u2514\u2500\u2500 {sampleN}-tumor.bam.bai\n</code></pre>"},{"location":"pipeline-details/output/#3-germline","title":"3. <code>germline</code>","text":"<p>This folder contains the output from the GATK Best Practices pipeline to obtain germline variants with a few alterations detailed below. Briefly, joint SNP and INDEL variant detection is conducted across all samples included in a pipeline run using the GATK Haplotypcaller under default settings. Raw variants are then subsequently filtered based on several GATK annotations: \\ A strict set of criteria (QD &lt; 2.0, FS &gt; 60.0, MQ &lt; 40.0, MQRankSum &lt; -12.5, ReadPosRankSum &lt; -8.0 for SNPs; QD &lt; 2.0, FS &gt; 200.0, ReadPosRankSum &lt; -20.0 for INDELs) generates the 'combined.strictFilter.vcf'.</p> <p>This call set is highly stringent, maximizing the true positive rate at the expense of an elevated false negative rate. This call set is really only intended for more general population genetic scale analyses (e.g., burden tests, admixture, linkage/pedigree based analysis, etc.) where false positives can be significantly confounding.</p> <p>In case of human sequence data, a basic analyses of sample relatedness and ancestry (e.g., % European, African, etc.) is also performed using somalier.</p> <p>The output folder looks like:</p> <pre><code>germline/\n\u251c\u2500\u2500 gVCFs\n.\n.\n.\n\u251c\u2500\u2500 somalier # only for hg38 genome\n\u2514\u2500\u2500 VCF\n</code></pre> <p>The <code>VCF</code> folder contains the final filtered germline variants (SNPs and INDELs) for all samples combined. The folder also contains raw variants for each sample, all samples combined, and also combined raw variants split by chromosome.</p> <pre><code>VCF/\n\u251c\u2500\u2500 by_chrom\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw_variants_byChrom.list\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 raw_variants.chr1.vcf.gz[.tbi]\n.   .\n.   .\n.   .\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 raw_variants.chrN.vcf.gz[.tbi]\n\u251c\u2500\u2500 indel.filterd.vcf.gz[.tbi]\n\u251c\u2500\u2500 {sample1}-normal.germline.vcf.gz[.tbi]\n\u251c\u2500\u2500 {sample1}-tumor.germline.vcf.gz[.tbi]\n.\n.\n.\n\u251c\u2500\u2500 {sampleN}-normal.germline.vcf.gz[.tbi]\n\u251c\u2500\u2500 {sampleN}-tumor.germline.vcf.gz[.tbi]\n\u251c\u2500\u2500 raw_variants.vcf.gz[.tbi]\n\u251c\u2500\u2500 snp.filtered.vcf.gz[.tbi]\n\u2514\u2500\u2500 snp_indel.filtered.vcf.gz[.tbi]\n</code></pre>"},{"location":"pipeline-details/output/#4-logfiles","title":"4. <code>logfiles</code>","text":"<p>This folder contains the snakemake log files and computational statistics for the XAVIER run. All the log files (i.e., standard output and error) for each individual step are in the <code>slurmfiles</code> folder. These logfiles are important to diagnose errors in case the pipeline fails.</p> <pre><code>logfiles/\n\u251c\u2500\u2500 master.log\n\u251c\u2500\u2500 mjobid.log\n\u251c\u2500\u2500 runtime_statistics.json\n\u251c\u2500\u2500 slurmfiles\n\u251c\u2500\u2500 snakemake.log\n\u251c\u2500\u2500 snakemake.log.jobby\n\u2514\u2500\u2500 snakemake.log.jobby.short\n</code></pre>"},{"location":"pipeline-details/output/#tumor-normal-pair","title":"Tumor-normal pair","text":""},{"location":"pipeline-details/output/#somatic_paired","title":"<code>somatic_paired</code>","text":"<p>This workflow calls somatic SNPs and INDELs using multiple variant detection algorithms. For each of these tools, variants are called in a paired tumor-normal fashion, with default settings. See Pipeline Details for more information about the tools used and their parameter settings.</p> <p>For each sample, the resulting VCF is fully annotated using VEP and converted to a MAF file using the vcf2maf tool. Resulting MAF files are found in <code>maf</code> folder within each caller's results directory (i.e., <code>mutect2_out</code>, <code>strelka_out</code>, etc.). Individual sample MAF files are then merged and saved in <code>merged_somatic_variants</code> directory.</p> <p>For Mutect2, we use a panel of normals (PON) developed from the ExAC (excluding TCGA) dataset, filtered for variants &lt;0.001 in the general population, and also including and in-house set of blacklisted recurrent germline variants that are not found in any population databases.</p> <p>For Copy Number Variants (CNVs), two tools are employed in tandem. First, Control-FREEC is run with default parameters. This generates pileup files that can be used by Sequenza, primarily for jointly estimating contamination and ploidy. These value are used to run Freec a second time for improved performance.</p> <p>The output directory should look like:</p> <pre><code>somatic_paired/\n\u251c\u2500\u2500 CNV # only if CNVs analyzed\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 freec_out\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sequenza_out\n\u251c\u2500\u2500 ffpe_filter # only if FFPE filter applied\n\u251c\u2500\u2500 qc\n\u2514\u2500\u2500 SNP_Indels\n    \u251c\u2500\u2500 merged_somatic_variants\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cohort_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 maf # Final merged MAFs for each sample\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 mutect2_out\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 chrom_split\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cohort_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 maf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 pileup_summaries\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 read_orientation_data\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 mutect_out\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 maf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 .\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 .\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 strelka_out\n    \u251c\u2500\u2500 vardict_out\n    \u2514\u2500\u2500 varscan_out\n</code></pre>"},{"location":"pipeline-details/output/#tumor-only","title":"Tumor-only","text":""},{"location":"pipeline-details/output/#somatic_tumor_only","title":"<code>somatic_tumor_only</code>","text":"<p>In general, the tumor-only pipeline is a stripped down version of the tumor-normal pipeline. We only run MuTect2, Mutect, and VarDict for somatic variant detection, with the same PON and filtering as described above for the tumor-normal pipeline.</p> <pre><code>somatic_tumor_only/\n\u251c\u2500\u2500 ffpe_filter # only if FFPE filter applied\n\u251c\u2500\u2500 qc\n\u2514\u2500\u2500 SNP_Indels\n    \u251c\u2500\u2500 merged_somatic_variants\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cohort_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 maf # Final merged MAFs for each sample\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 mutect2_out\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 chrom_split\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cohort_summary\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 maf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 read_orientation_data\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 mutect_out\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 maf\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 .\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 .\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 vardict_out\n    \u2514\u2500\u2500 varscan_out\n</code></pre>"},{"location":"pipeline-details/overview/","title":"Pipeline Overview","text":"<p><sup>Workflow diagram of the XAVIER: the pipeline is composed of a series of data processing steps to trim, align, and recalibrate reads prior to calling variants. These data processing steps closely follow GATK's best pratices for cleaning up raw alignments. The pipeline also consists of a series of comprehensive quality-control steps.</sup></p>"},{"location":"pipeline-details/settings/","title":"Settings","text":"<p>This page contains details of the settings used for different tools in the pipeline</p>"},{"location":"pipeline-details/settings/#somatic-paired-variant-calling","title":"Somatic paired variant calling","text":"<p>Tool</p><p>Step</p><p>Argument</p><p>Description</p><p>Resource</p><p>mutect2</p><p>calling</p><p>--panel-of-normals</p><p>1000 Genomes with COSMIC and ClinVar samples removed</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p><p></p><p>--germline-resource</p><p>GATK Bundle; reheadered to match genome fasta</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/GNOMAD/somatic-hg38-af-only-gnomad.hg38.vcf.gz</p><p>-normal</p><p>BAM file for paired normal sample</p><p></p><p>filter</p><p>--ob-priors</p><p>from GATK LearnReadOrientationModel (uses f1r2 file output during calling)</p><p></p><p>--contamination-table</p><p>from GATK CalculateContamination (uses pileup of tumor AND normal bam)</p><p></p><p>strelka</p><p>calling</p><p>--exome</p><p>Preset filters for exome data</p><p></p><p>mutect</p><p>--normal_panel</p><p>1000 Genomes with COSMIC and ClinVar samples removed</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p><p></p><p>--cosmic</p><p>COSMIC version 82</p><p>/data/CCBR_Pipeliner/db/PipeDB/lib/COSMIC_82_hg38.vcf.gz</p><p>--dbsnp</p><p>dbSNP version 138</p><p>/fdb/GATK_resource_bundle/hg38bundle/dbsnp_138.hg38.vcf.gz</p><p>-rf BadCigar</p><p>removes bad cigar strings, similar to https://gatk.broadinstitute.org/hc/en-us/articles/360037430171-GoodCigarReadFilter </p><p></p><p>vardict</p><p>-f 0.05</p><p>Minimum variant allele frequency threshold 0.05</p><p></p><p></p><p>--nosv</p><p>Turn off structural variant calling</p><p></p><p>-t</p><p>Remove duplicated reads</p><p></p><p>-Q 20 </p><p>Reads with quality &lt; 20 are removed</p><p></p><p>-c 1 </p><p>Column of targets BED file with chromosome</p><p></p><p>-S 2 </p><p>Column of targets BED file with start position</p><p></p><p>-E 3</p><p>Column of targets BED file with end position</p><p></p><p>var2vcf</p><p>-d 10</p><p>Min total depth</p><p></p><p>-v 6</p><p>Min variant depth</p><p></p><p>-M</p><p>If set, output only candidate somatic</p><p></p><p>-S</p><p>exclude variants that fail filters</p><p></p><p>filter</p><p>--exclude 'STATUS=\"Germline\" | STATUS=\"LikelyLOH\" | STATUS=\"AFDiff\"'</p><p>Removes variants with certain flags from vardict; (1) Germline: detected in germline sample (pass all quality parameters); (2) LikelyLOH: detected in germline but either lost in tumor OR 20-80% in germline, but increased to 1-opt_V (95%); (3) AFDiff: detected in tumor (pass quality parameters) and present in germline but didn\u2019t pass quality parameters.</p><p></p><p>varscan</p><p>pileup</p><p>-d 100000 -q 15 -Q 15 </p><p>samtools mpileup arguments; max depth of 100,000; min mapping quality of 15; min base quality of 15</p><p></p><p></p><p>calling</p><p>--strand-filter 0 </p><p>Do not ignore variants with &gt;90% support on one strand</p><p></p><p>--min-var-freq 0.01</p><p>Minimum variant allele frequency threshold 0.01</p><p></p><p>--output-vcf 1</p><p>Output in VCF format</p><p></p><p>--variants 1</p><p>Report only variant (SNP/indel) positions</p><p></p><p>all</p><p>GATK SelectVariants</p><p>--exclude-filtered</p><p>Removes non-PASS variants</p><p></p><p></p><p>--discordance</p><p>Remove variants found in supplied file (same as panel-of-normals file)</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p>"},{"location":"pipeline-details/settings/#somatic-tumor-only-variant-calling","title":"Somatic tumor-only variant calling","text":"<p>Tool</p><p>Step</p><p>Argument</p><p>Description</p><p>Resource</p><p>mutect2</p><p>calling</p><p>--panel-of-normals</p><p>1000 Genomes with COSMIC and ClinVar samples removed</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p><p></p><p>--germline-resource</p><p>GATK Bundle; reheadered to match genome fasta</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/GNOMAD/somatic-hg38-af-only-gnomad.hg38.vcf.gz</p><p>filter</p><p>--ob-priors</p><p>from GATK LearnReadOrientationModel (uses f1r2 file output during calling)</p><p></p><p>--contamination-table</p><p>from GATK CalculateContamination (uses pileup of tumor bam)</p><p></p><p>mutect</p><p>calling</p><p>--normal_panel</p><p>1000 Genomes with COSMIC and ClinVar samples removed</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p><p></p><p>--cosmic</p><p>COSMIC version 82</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p><p>--dbsnp</p><p>dbSNP version 138</p><p>/fdb/GATK_resource_bundle/hg38bundle/dbsnp_138.hg38.vcf.gz</p><p>-rf BadCigar</p><p>removes bad cigar strings, similar to https://gatk.broadinstitute.org/hc/en-us/articles/360037430171-GoodCigarReadFilter </p><p></p><p>vardict</p><p>-f 0.05</p><p>Minimum variant allele frequency threshold 0.05</p><p></p><p></p><p>-x 500</p><p>Nucleotides to extend</p><p></p><p>--nosv</p><p>Turn off structural variant calling</p><p></p><p>-t</p><p>Remove duplicated reads</p><p></p><p>-Q 20 </p><p>Reads with map quality &lt; 20 are removed</p><p></p><p>-c 1 </p><p>Column of targets BED file with chromosome</p><p></p><p>-S 2 </p><p>Column of targets BED file with start position</p><p></p><p>-E 3</p><p>Column of targets BED file with end position</p><p></p><p>var2vcf</p><p>-d 10</p><p>Min total depth</p><p></p><p>-v 6</p><p>Min variant depth</p><p></p><p>-S</p><p>exclude variants that fail filters</p><p></p><p>varscan</p><p>pileup</p><p>-d 100000 -q 15 -Q 15 </p><p>samtools mpileup arguments; max depth of 100,000; min mapping quality of 15; min base quality of 15</p><p></p><p></p><p>calling</p><p>--strand-filter 0 </p><p>Do not ignore variants with &gt;90% support on one strand</p><p></p><p>--min-var-freq 0.01</p><p>Minimum variant allele frequency threshold 0.01</p><p></p><p>--output-vcf 1</p><p>Output in VCF format</p><p></p><p>--variants 1</p><p>Report only variant (SNP/indel) positions</p><p></p><p>all</p><p>GATK SelectVariants</p><p>--exclude-filtered</p><p>Removes non-PASS variants</p><p></p><p></p><p>--discordance</p><p>Remove variants found in supplied file (same as panel-of-normals file)</p><p>/data/CCBR_Pipeliner/Pipelines/XAVIER/resources/hg38/PON/hg38.noCOSMIC_ClinVar.pon.vcf.gz</p>"},{"location":"pipeline-details/tools/","title":"Tools","text":"<p>This table lists information about the steps performed, tools used, and their details.</p> <p>Module</p><p>Category</p><p>Analysis Type</p><p>Software</p><p>Version</p><p>Rule File(s)</p><p>Preprocessing</p><p>Preprocessing</p><p>trim</p><p>Trimmomatic</p><p>0.39</p><p>trim_map_preprocess.smk</p><p>map</p><p>bwa</p><p>0.7.17</p><p>trim_map_preprocess.smk</p><p>markdup</p><p>samblaster </p><p>0.1.25</p><p>trim_map_preprocess.smk</p><p>GATK4 Best Practices</p><p>GATK4</p><p>4.2.2</p><p></p><p></p><p></p><p>SNP/Indel Calling</p><p>Mutect2</p><p>GATK 4.2.2</p><p>somatic_snps.paired.smk, somatic_snps.tumor_only.smk</p><p>Strelka</p><p>2.9.0</p><p>somatic_snps.paired.smk</p><p>VarScan</p><p>2.4.3</p><p>somatic_snps.paired.smk, somatic_snps.tumor_only.smk</p><p>Mutect</p><p>1.1.7</p><p>somatic_snps.paired.smk, somatic_snps.tumor_only.smk</p><p>VarDict</p><p>1.4</p><p>somatic_snps.paired.smk, somatic_snps.tumor_only.smk</p><p>FFPE Artifact Filter</p><p>SOBDetector</p><p>1.0.4</p><p>ffpe.smk</p><p>Consensus SNP/Indels</p><p>GATK3 CombineVariants</p><p>GATK_3.8-1</p><p>somatic_snps.common.smk</p><p>Somatic Copy Number Variation (CNV)</p><p>CNV</p><p>Control-FREEC</p><p>11.5</p><p>somatic_snps.paired.smk</p><p>Sequenza</p><p></p><p>somatic_snps.paired.smk</p><p>Somatic Analysis</p><p>Annotate</p><p>vcf2maf</p><p></p><p>somatic_snps.common.smk</p><p>Germline</p><p>Germline SNV Calling</p><p>Germline Variants</p><p>HaplotypeCaller</p><p>GATK_4.2.2</p><p>germline.smk</p><p>Germline Analysis</p><p>Ancestry</p><p>Somalier</p><p></p><p>qc.smk</p><p>Relatedness</p><p>Somalier</p><p></p><p>qc.smk</p><p>QC Metrics</p><p>QC</p><p>depth</p><p>qualimap</p><p>2.2.1</p><p>qc.smk</p><p>report</p><p>multiqc</p><p>1.11 </p><p>qc.smk</p><p>base quality</p><p>FastQC</p><p>0.11.9</p><p>qc.smk</p><p>contamination</p><p>Fastq Screen</p><p>0.14.1</p><p>qc.smk</p><p>kraken</p><p>2.1.2</p><p>qc.smk</p><p>variant quality</p><p>vcftools stat</p><p>0.1.16</p><p>qc.smk</p><p>bcftools_stat</p><p>1.9</p><p>qc.smk</p><p>variant effect</p><p>SNPeff</p><p>4.3t</p><p>qc.smk</p><p>General</p><p>General</p><p>R scripts</p><p>R</p><p>4.1</p><p>general</p><p>variant wrangling</p><p>bcftools</p><p>1.9</p><p>general</p><p>vcftools</p><p>0.1.16</p><p>general</p><p>alignment wrangling</p><p>samtools</p><p>1.8</p><p>general</p><p>Orchestration</p><p>Orchestration</p><p>Containerization</p><p>singularity</p><p>3.8.5</p><p>Orchestration</p><p>Workflow managemanet</p><p>snakemake</p><p>6.8.2</p><p>Orchestration</p>"},{"location":"usage/cache/","title":"<code>xavier cache</code>","text":""},{"location":"usage/cache/#1-about","title":"1. About","text":"<p>The <code>xavier</code> executable is composed of several inter-related sub commands. Please see <code>xavier -h</code> for all available options.</p> <p>This part of the documentation describes options and concepts for <code>xavier cache</code> sub command in more detail. With minimal configuration, the <code>cache</code> sub command enables you to cache remote resources for the xavier pipeline. Caching remote resources allows the pipeline to run in an offline mode. The cache sub command can also be used to pull our pre-built reference bundles onto a new cluster or target system.</p> <p>The cache sub command creates local cache on the filesysytem for resources hosted on DockerHub or AWS S3. These resources are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created and re-used. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that its cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub.</p>"},{"location":"usage/cache/#2-synopsis","title":"2. Synopsis","text":"<p>Coming Soon!</p>"},{"location":"usage/gui/","title":"Getting started","text":""},{"location":"usage/gui/#1-synopsis","title":"1. Synopsis","text":"<p>XAVIER pipeline can be executed from either using the graphical user interface (GUI) or the command line interface (CLI). GUI offers a more interactive way for the user to provide input and adjust parameter settings. This part of the documentation describes how to run xavier using the GUI (with screenshots). See Command Line tab to read more about the <code>xavier</code> executable and running XAVIER pipeline using the CLI.</p>"},{"location":"usage/gui/#2-setting-up-xavier","title":"2. Setting up XAVIER","text":""},{"location":"usage/gui/#21-login-to-cluster","title":"2.1 Login to cluster","text":"<pre><code># Setup Step 1.) ssh into cluster's head node\n# example below for Biowulf cluster\nssh -Y $USER@biowulf.nih.gov\n</code></pre>"},{"location":"usage/gui/#22-grab-an-interactive-node","title":"2.2 Grab an interactive node","text":"<pre><code># Setup Step 2.) Please do not run XAVIER on the head node!\n# Grab an interactive node first\nsinteractive --time=12:00:00 --mem=8gb  --cpus-per-task=4\n</code></pre>"},{"location":"usage/gui/#23-load-ccbrpipeliner-module","title":"2.3 Load <code>ccbrpipeliner</code> module","text":"<p>NOTE: <code>ccbrpipeliner</code> is a custom module created on biowulf which contains various NGS data analysis pipelines developed, tested, and benchmarked by experts at CCBR.</p> <pre><code># Setup Step 3.) Add ccbrpipeliner module\nmodule purge # to reset the module environment\nmodule load ccbrpipeliner\n</code></pre> <p>If the module was loaded correctly, the greetings message should be displayed.</p> <pre><code>[+] Loading ccbrpipeliner  5  ...\n###########################################################################\n                                CCBR Pipeliner\n###########################################################################\n    \"ccbrpipeliner\" is a suite of end-to-end pipelines and tools\n    Visit https://github.com/ccbr for more details.\n    Pipelines are available on BIOWULF and FRCE.\n    Tools are available on BIOWULF, HELIX and FRCE.\n\n    The following pipelines/tools will be loaded in this module:\n\n    RENEE v2.5 https://ccbr.github.io/RENEE/\n    XAVIER v3.0 https://ccbr.github.io/XAVIER/\n    CARLISLE v2.4 https://ccbr.github.io/CARLISLE/\n    CHAMPAGNE v0.2 https://ccbr.github.io/CHAMPAGNE/\n    CRUISE v0.1 https://ccbr.github.io/CRUISE/\n\n    spacesavers2 v0.10 https://ccbr.github.io/spacesavers2/\n    permfix v0.6 https://github.com/ccbr/permfix\n###########################################################################\nThank you for using CCBR Pipeliner\n###########################################################################\n</code></pre> <p>To check the current version of XAVIER, enter:</p> <pre><code>xavier --version\n</code></pre>"},{"location":"usage/gui/#3-running-xavier","title":"3. Running XAVIER","text":""},{"location":"usage/gui/#31-launching-xavier-gui","title":"3.1 Launching XAVIER GUI","text":"<p>To run the XAVIER pipeline from the GUI, simply enter:</p> <pre><code>xavier gui\n</code></pre> <p>and it will launch the XAVIER window.</p> <p>Note: Please wait until <code>window created!</code> message appears on the terminal.</p> <p></p>"},{"location":"usage/gui/#32-folder-paths-and-reference-genomes","title":"3.2 Folder paths and reference genomes","text":"<p>To enter the location of the input folder containing FASTQ files and the location where the output folders should be created, either simply type the absolute paths</p> <p></p> <p>or use the Browse tab to choose the input and output directories</p> <p></p> <p>Next, from the drop down menu select the reference genome (hg38/mm10)</p> <p></p> <p>and enter a job name of this run.</p> <p></p>"},{"location":"usage/gui/#33-analysis-mode","title":"3.3 Analysis mode","text":"<p>XAVIER pipeline can be run in two different modes:\\ (A) Tumor-normal pair \\ (B) Tumor-only</p>"},{"location":"usage/gui/#33a-tumor-normal-pair-analysis","title":"3.3a Tumor-normal pair analysis","text":"<p>In case of tumor-normal pairs, a tab-delimited text file is neeed that contains the list of normal and tumor samples. For example,</p> <pre><code>Normal  Tumor\nsample1-normal     sample1-tumor\nsample2-normal     sample2-tumor\nsample3-normal     sample3-tumor\nsample4-normal     sample4-tumor\n</code></pre> <p>Similar to input and output folder paths, either type the path to the pairsInfo.txt file or use the Browse tab.</p> <p>In case of paired mode, XAVIER can also perform copy number variants (CNV) analysis.</p> <p></p>"},{"location":"usage/gui/#33b-tumor-only-analysis","title":"3.3b Tumor-only analysis","text":"<p>In case the paired normal samples are unavailable, XAVIER pipeline can be run in tumor-only mode which does not require paired samples information. However, in the absence of matching normal samples, CNV analysis is also unavailable.</p> <p></p>"},{"location":"usage/gui/#34-submit-xavier-job","title":"3.4 Submit XAVIER job","text":"<p>After all the information is filled out, press Submit.</p> <p>If the pipeline detects no errors and the run was submitted, a new window appears that has the output of a \"dry-run\" which summarizes each step of the pipeline.</p> <p></p> <p>Click OK</p> <p>A dialogue box will popup to confirm submitting the job to slurm.</p> <p></p> <p>Click Yes</p> <p>The dry-run output will be displayed again and the master job will be submitted. An email notification will be sent out when the pipeline starts and ends.</p> <p>The XAVIER gui will ask to submit another job.</p> <p></p> <p>Click Yes to start again or No to close the XAVIER gui.</p>"},{"location":"usage/gui/#35-additional-settings","title":"3.5 Additional settings","text":"<p>Users can input certain additional settings for the pipeline run including running an additional step to correct strand orientation bias in Formalin-Fixed Paraffin-Embedded (FFPE) samples and to provide a custom exome targets BED file. This file can be obtained from the manufacturer of the target capture kit that was used.</p> <p></p> <p></p>"},{"location":"usage/gui/#4-special-instructions-for-biowulf","title":"4. Special instructions for Biowulf","text":"<p>XAVIER GUI natively uses the X11 Window System to run XAVIER pipeline and display the graphics on a personal desktop or laptop. However, if running XAVIER specifically on NIH's Biowulf cluster, the HPC staff recommends NoMachine (NX) to run graphics applications.</p> <p>Please see details here on how to install and connect to Biowulf on your local computer using NoMachine.</p> <p>Once connected to Biowulf using NX, right click to open a terminal connection</p> <p></p> <p>and start an interactive session.</p> <p></p> <p>Similar to the instructions above, load <code>ccbrpipeliner</code> module and enter <code>xavier gui</code> to launch the XAVIER gui.</p> <p></p>"},{"location":"usage/run/","title":"<code>xavier run</code>","text":""},{"location":"usage/run/#1-about","title":"1. About","text":"<p>The <code>xavier</code> executable is composed of several inter-related sub commands. Please see <code>xavier -h</code> for all available options.</p> <p>This part of the documentation describes options and concepts for <code>xavier run</code> sub command in more detail. With minimal configuration, the <code>run</code> sub command enables you to start running xavier pipeline.</p> <p>Setting up the xavier pipeline is fast and easy! In its most basic form, <code>xavier run</code> only has four required inputs.</p>"},{"location":"usage/run/#2-synopsis","title":"2. Synopsis","text":"<pre><code>$ xavier run [--help] \\\n                   [--mode {local, slurm}] \\\n                   [--job-name JOB_NAME] \\\n                   [--callers {mutect2,mutect,strelka, ...}] \\\n                   [--pairs PAIRS] \\\n                   [--ffpe] \\\n                   [--cnv] \\\n                   [--silent] \\\n                   [--singularity-cache SINGULARITY_CACHE] \\\n                   [--sif-cache SIF_CACHE] \\\n                   [--threads THREADS] \\\n                   --runmode {init, dryrun, run} \\\n                   --input INPUT [INPUT ...] \\\n                   --output OUTPUT \\\n                   --genome {hg38, ...} \\\n                   --targets TARGETS\n</code></pre> <p>The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets.</p> <p>A user must provide a list of FastQ or BAM files (globbing is supported) to analyze via <code>--input</code> argument, an output directory to store results via <code>--output</code> argument, an exome targets BED file for the samples' capture kit, and select reference genome for alignment and annotation via the <code>--genome</code> argument.</p> <p>Use you can always use the <code>-h</code> option for information on a specific command.</p>"},{"location":"usage/run/#21-required-arguments","title":"2.1 Required Arguments","text":"<p>Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code.</p> <p><code>--input INPUT [INPUT ...]</code></p> <p>Input FastQ or BAM file(s) to process. type: file(s)</p> <p>One or more FastQ files can be provided. The pipeline does NOT support single-end WES data. Please provide either a set of FastQ files or a set of BAM files. The pipeline does NOT support processing a mixture of FastQ files and BAM files. From the command-line, each input file should separated by a space. Globbing is supported! This makes selecting FastQ files easy. Input FastQ files should be gzipp-ed.</p> <p>Example: <code>--input tests/data/*.R?.fastq.gz</code></p> <p>Example: <code>--input /data/CCBR_Pipeliner/testdata/XAVIER/human_subset/*.R?.fastq.gz</code></p> <p><code>--output OUTPUT</code></p> <p>Path to an output directory. type: path</p> <p>This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be initialized automatically.</p> <p>Example: <code>--output /data/$USER/WES_hg38</code></p> <p><code>--runmode {init,dryrun,run}</code> `</p> <p>Execution Process. type: string</p> <p>User should initialize the pipeline folder by first running <code>--runmode init</code> User should then perform a dry-run to list all steps the pipeline will take<code>--runmode dryrun</code> User should then perform the full run <code>--runmode run</code></p> <p>Example: <code>--runmode init</code> THEN <code>--runmode dryrun</code> THEN <code>--runmode run</code></p> <p><code>--genome {hg38, custom.json}</code></p> <p>Reference genome. type: string/file</p> <p>This option defines the reference genome for your set of samples. On Biowulf, xavier does comes bundled with pre built reference files for human samples; however, it is worth noting that the pipeline does accept a pre-built resource bundle pulled with the cache sub command (coming soon). Currently, the pipeline only supports the human reference hg38; however, support for mouse reference mm10 will be added soon.</p> <p>Pre built Option Here is a list of available pre built genomes on Biowulf: hg38, mm10.</p> <p>Custom Option For users running the pipeline outside of Biowulf, a pre-built resource bundle can be pulled with the cache sub command (coming soon). Please supply the custom reference JSON file that was generated by the cache sub command.</p> <p>Example: <code>--genome hg38</code> OR <code>--genome /data/${USER}/hg38/hg38.json</code></p> <p><code>--targets TARGETS</code></p> <p>Exome targets BED file. type: file</p> <p>This file can be obtained from the manufacturer of the target capture kit that was used.</p> <p>If not provided, the default targets file from the genome config is used</p> <p>Example: <code>--targets resources/Agilent_SSv7_allExons_hg38.bed</code> &gt; Example: <code>--targets resources/SureSelect_mm10_sorted.bed</code></p>"},{"location":"usage/run/#22-options","title":"2.2 Options","text":"<p>Each of the following arguments are optional and do not need to be provided.</p> <p><code>-h, --help</code></p> <p>Display Help. type: boolean flag</p> <p>Shows command's synopsis, help message, and an example command</p> <p>Example: <code>--help</code></p> <p><code>--silent</code></p> <p>Silence standard output. type: boolean flag</p> <p>Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned.</p> <p>Example: <code>--silent</code></p> <p><code>--mode {local,slurm}</code></p> <p>Execution Method. type: string default: slurm</p> <p>Execution Method. Defines the mode or method of execution. Valid mode options include: local or slurm.</p> <p>local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode.</p> <p>slurm The slurm execution method will submit jobs to a cluster using a singularity backend. It is recommended running xavier in this mode as execution will be significantly faster in a distributed environment.</p> <p>Example: <code>--mode slurm</code></p> <p><code>--job-name JOB_NAME</code></p> <p>Set the name of the pipeline's master job. type: string &gt; default: pl:xavier</p> <p>When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:xavier\".</p> <p>Example: <code>--job-name xavier_run1</code></p> <p><code>--callers CALLERS [CALLERS ...]</code></p> <p>Variant Callers. type: string(s) &gt; default: mutect2, mutect, strelka, vardict, varscan</p> <p>List of variant callers to detect mutations. Please select from one or more of the following options: [mutect2, mutect, strelka, vardict, varscan]. Defaults to using all variant callers.</p> <p>Example: <code>--callers mutect2 strelka varscan</code></p> <p><code>--pairs PAIRS</code></p> <p>Tumor normal pairs file. type: file</p> <p>This tab delimited file contains two columns with the names of tumor and normal pairs, one per line. The header of the file needs to be <code>Tumor</code> for the tumor column and <code>Normal</code> for the normal column. The base name of each sample should be listed in the pairs file. The base name of a given sample can be determined by removing the following extension from the sample's R1 FastQ file: <code>.R1.fastq.gz</code>. Contents of example pairs file:</p> <pre><code>Normal    Tumor\nSample4_CRL1622_S31   Sample10_ARK1_S37\nSample4_CRL1622_S31   Sample11_ACI_158_S38\n</code></pre> <p>Example: <code>--pairs /data/$USER/pairs.tsv</code></p> <p><code>--ffpe</code></p> <p>Apply FFPE correction. type: boolean flag</p> <p>Runs an additional steps to correct strand orientation bias in Formalin-Fixed Paraffin-Embedded (FFPE) samples. Do NOT use this option with non-FFPE samples.</p> <p>Example: <code>--ffpe</code></p> <p><code>--cnv</code></p> <p>Call copy number variations (CNVs). type: boolean flag</p> <p>CNVs will only be called from tumor-normal pairs. If this option is provided without providing a --pairs file, CNVs will NOT be called.</p> <p>Example: <code>--cnv</code></p> <p><code>--singularity-cache SINGULARITY_CACHE</code></p> <p>Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: <code>--output OUTPUT/.singularity</code></p> <p>Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the <code>--output</code> argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the <code>--sif-cache</code> option to create a shareable resource.</p> <p>Example: <code>--singularity-cache /data/$USER/.singularity</code></p> <p><code>--sif-cache SIF_CACHE</code></p> <p>Path where a local cache of SIFs are stored. type: path</p> <p>Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The <code>xavier cache</code> subcommand can be used to create a local SIF cache. Please see <code>xavier cache</code> for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running xavier with this option when ever possible.</p> <p>Example: <code>--singularity-cache /data/$USER/SIFs</code></p> <p><code>--threads THREADS</code></p> <p>Max number of threads for each process. type: int default: 2</p> <p>Max number of threads for each process. This option is more applicable when running the pipeline with <code>--mode local</code>. It is recommended setting this value to the maximum number of CPUs available on the host machine.</p> <p>Example: <code>--threads 12</code></p>"},{"location":"usage/run/#3-example","title":"3. Example","text":"<pre><code># Step 1.) Grab an interactive node\n# Do not run on head node!\nsinteractive --mem=8g --cpus-per-task=4\nmodule purge\nmodule load ccbrpipeliner\n\n# Step 2A.) Initialize the all resources to the output folder\nxavier run --input tests/data/*.R?.fastq.gz \\\n                 --output /data/$USER/xavier_hg38 \\\n                 --genome hg38 \\\n                 --targets Agilent_SSv7_allExons_hg38.bed \\\n                 --mode slurm \\\n                 --runmode init\n\n# Step 2B.) Dry-run the pipeline\nxavier run --input tests/data/*.R?.fastq.gz \\\n                 --output /data/$USER/xavier_hg38 \\\n                 --genome hg38 \\\n                 --targets Agilent_SSv7_allExons_hg38.bed \\\n                 --mode slurm \\\n                 --runmode dryrun\n\n# Step 2C.) Run the XAVIER pipeline\n# The slurm mode will submit jobs to the cluster.\n# It is recommended running xavier in this mode.\nxavier run --input tests/data/*.R?.fastq.gz \\\n                 --output /data/$USER/xavier_hg38 \\\n                 --genome hg38 \\\n                 --targets Agilent_SSv7_allExons_hg38.bed \\\n                 --mode slurm \\\n                 --runmode run\n</code></pre> <p>The example dataset in <code>tests/data</code> in this repository is a very small subsampled dataset, and some steps of the pipeline fail due to the small size (CNV callling, somalier, etc). We have a larger subsample (25% of a full human dataset) available on Biowulf if you would like to test the full functionality of the pipeline: <code>/data/CCBR_Pipeliner/testdata/XAVIER/human_subset/*.R?.fastq.gz</code></p>"},{"location":"usage/unlock/","title":"<code>xavier unlock</code>","text":""},{"location":"usage/unlock/#1-about","title":"1. About","text":"<p>The <code>xavier</code> executable is composed of several inter-related sub commands. Please see <code>xavier -h</code> for all available options.</p> <p>This part of the documentation describes options and concepts for <code>xavier unlock</code> sub command in more detail. With minimal configuration, the <code>unlock</code> sub command enables you to unlock a pipeline output directory.</p> <p>If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: <code>Error: Directory cannot be locked</code>.</p> <p>Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command.</p> <p>Unlocking xavier pipeline output directory is fast and easy! In its most basic form, <code>xavier unlock</code> only has one required input.</p>"},{"location":"usage/unlock/#2-synopsis","title":"2. Synopsis","text":"<pre><code>$ xavier unlock [-h] --output OUTPUT\n</code></pre> <p>The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets.</p> <p>A user must provide an output directory to unlock via <code>--output</code> argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it.</p> <p>Use you can always use the <code>-h</code> option for information on a specific command.</p>"},{"location":"usage/unlock/#21-required-arguments","title":"2.1 Required Arguments","text":"<p><code>--output OUTPUT</code></p> <p>Output directory to unlock. type: path</p> <p>Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: <code>--output /data/$USER/WES_hg38</code></p>"},{"location":"usage/unlock/#22-options","title":"2.2 Options","text":"<p>Each of the following arguments are optional and do not need to be provided.</p> <p><code>-h, --help</code></p> <p>Display Help. type: boolean</p> <p>Shows command's synopsis, help message, and an example command</p> <p>Example: <code>--help</code></p>"},{"location":"usage/unlock/#3-example","title":"3. Example","text":"<pre><code># Step 0.) Grab an interactive node (do not run on head node)\nsinteractive --mem=8g -N 1 -n 4\nmodule purge\nmodule load ccbrpipeliner\n\n# Step 1.) Unlock a pipeline output directory\nxavier unlock --output /data/$USER/xavier_hg38\n</code></pre>"}]}